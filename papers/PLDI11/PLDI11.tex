% !TEX TS-program = pdflatex
% !TEX encoding = UTF-8 Unicode

\documentclass[preprint]{sigplanconf}

\usepackage{graphicx,listings,fixltx2e,lambda,array,multirow,color}




\begin{document}
\conferenceinfo{PLDI 2011}{June 4--8, 2011, San Jose, CA, USA.}
\copyrightyear{2010}

\preprintfooter{PLDI 2011}
\titlebanner{DRAFT---Do not distribute}

\title{Parakeet: General Purpose GPU Programming with High Level Array
Languages}
\authorinfo{Anonymous}{Anonymous}{Anonymous}
\lstset{ 
basicstyle=\small\footnotesize\ttfamily, % Standardschrift
numbers=left,                   % where to put the line-numbers
stepnumber=1,                   % the step between two line-numbers. If it's 1 each line 
numbersep=12pt,                  % how far the line-numbers are from the code
showspaces=false,               % show spaces adding particular underscores
showstringspaces=false,         % underline spaces within strings
showtabs=false,                 % show tabs within strings adding particular underscores
tabsize=2,	                % sets default tabsize to 2 spaces
captionpos=b,                   % sets the caption-position to bottom
breaklines=true,                % sets automatic line breaking
breakatwhitespace=false,        % sets if automatic breaks should only happen at whitespace
keywordstyle=\color{black}\bf,
%morekeywords={til,sum,sqrt,all, min, each, where, while,+,*,-,avg},            % if you want to add more keywords to the set
moredelim=[is][\itshape]{/*}{*/},
linewidth={\textwidth},
xleftmargin=20pt,
%framexleftmargin=17pt,
%framexrightmargin=5pt,
%framexbottommargin=4pt,
}

% define some useful commands to use in language specification 
\newcommand{\WITH}{\impfnt{with}}
\newcommand{\VALUES}{\impfnt{values}}
\newcommand{\MAP}{\impfnt{map}}
\newcommand{\REDUCE}{\impfnt{reduce}}
\newcommand{\ALLPAIRS}{\impfnt{allpairs}}
\newcommand{\CAST}{\impfnt{cast}}
\newcommand{\APPLY}{\impfnt{apply}}
\newcommand{\INDEX}{\impfnt{index}}
\newcommand{\SCAN}{\impfnt{scan}}

\maketitle



\begin{abstract}
Contemporary GPUs offer staggering performance potential, which has
led to an explosion in recent work on enabling the execution of
general-purpose programs on GPUs. Despite various advances in making
general-purpose GPU programming easier, the two commonly used
frameworks (NVIDIA's CUDA and OpenCL) are cumbersome and require
detailed architectural knowledge on the part of the programmer to
fully harness this performance potential.

The potential performance advantage over contemporary CPUs that GPUs
offer is due in large part to their having been heavily optimized for
data parallel workloads.  Array-oriented programming--which encourages
the use of data parallel array operators (e.g.\ map, reduce, and
scan), and de-emphasizes the importance of explicit loops--is thus a
natural model for high level programming of GPUs.

We present Parakeet, an intelligent runtime for executing high level
array-oriented programs on GPUs.  The heart of Parakeet is an
interpreter, which upon reaching an array operator synthesizes and
executes a GPU program to implement that operator. Parakeet takes
advantage of runtime information both to choose between different
possible implementations of a GPU program and to set execution
parameters which greatly impact performance. Parakeet transparently
moves data to and from the GPU and performs garbage collection of GPU
memory.

We evaluate our system on two standard benchmarks: Black-Scholes
option pricing, and K-Means clustering.  We compare high level
array-oriented implementations to hand-written tuned GPU versions from
the CUDA SDK and the Rodinia GPU benchmark suite. Despite having
orders of magnitude less code, the high level
versions perform competitively when executed by Parakeet.
\end{abstract}

\section{Introduction}

Contemporary GPUs boast massive performance potential--in some cases orders of
magnitude higher performance per dollar than that of CPUs--and most desktop
systems today come with graphics processors.  This has led to an explosion in
recent work on enabling the execution of general-purpose programs on GPUs
(GPGPU) in order to harness this performance for non-graphics tasks
\cite{Cata10,Main10,Muns10,NvidCU,Sven08,Tard06}. Despite this, the two widely
used GPGPU frameworks--NVIDIA's CUDA \cite{NvidCU} and OpenCL
\cite{Muns10}--require the programmer to use extensive knowledge of low level
architectural details in order to fully harness the performance potential.
While this might be acceptable for well-trained systems hackers, professionals
in other domains such as the natural sciences who could potentially
benefit from the performance boost from using GPUs are unable to do so.

% Enable programmers to write GPGPU programs in productivity language
Our goal is to lower the barrier on GPGPU programming by
enabling programmers to use productivity languages for writing
efficient GPGPU programs. Productivity languages, such as Python or Matlab,
trade off program speed in order to allow programmers to write code more quickly
and with less effort than efficiency languages such as C.  We aim to deliver
both performance and ease of use by focusing on a specific class of parallel
computation: data parallelism.  Data parallelism is present in many important
algorithms and problem domains, e.g.\ machine learning and linear algebra, and
has been well studied in the literature (e.g.\ in \cite{Blel90}).  Data
parallelism is also a natural
model for GPU programming, as much of the performance advantage GPUs offer over
CPUs comes from their having been heavily optimized for data parallel workloads.
 We thus target languages with support for \emph{array-oriented programming}.
Array-oriented programming encourages the use of data parallel array operators
(such as map, reduce, and scan) for operating on first-class array types, and
de-emphasizes the importance of explicit loops.

% Heart of Parakeet - an interpreter for our high level IL w/ array ops
In this paper, we present Parakeet, an intelligent runtime for executing high
level array-oriented programs on GPUs.  The core of Parakeet is an interpreter
for its high level, typed intermediate language that includes native support for
array operators.  When the Parakeet interpreter reaches an array operator such
as {\tt Map}, it uses dynamic runtime information to decide whether to
synthesize a GPU program to implement that operator, taking advantage of the
GPU transparently to the programmer.

% Agnosticism
Parakeet is agnostic as to the source language used by the programmer, provided
that the language has support for the required array operators.  We have
implemented the first front end for the Parakeet system for Q, a high level,
dynamically typed array language derived from APL and widely used in financial
computing \cite{Borr08}. Parakeet's first back end is for PTX, the
pseudoassembly language used by NVIDIA GPUs \cite{NvidCU}.  We are currently
working on front ends for data parallel subsets of Matlab and Python, and plan
on supporting multicore CPUs in the future as well.

% Pipeline of execution
The pipeline of the execution of a program begins by the Parakeet system
intercepting function calls from the source language's interpreter and type
specializing them according to their argument types.  Parakeet then performs
various standard compiler optimizations on this IL form of the functions, as
well as fusion of array operators which is very beneficial for performance.
Parakeet then interprets the typed function, selectively executing array
operators either on the GPU or CPU. Parakeet synthesizes GPU
programs from a library of efficient array operator implementation skeletons,
with splice points where user-defined functions get inlined. Parakeet supports
the nesting of array operators, using heuristics to decide which level of
nesting should be compiled to GPU kernels, with the outer levels of nesting are
translated into iterative kernel invocations by our CPU runtime.  Data movement
and GPU garbage collection are handled transparently by Parakeet.

% Performance
We evaluate Parakeet on two standard benchmark programs: Black-Scholes option
pricing and K-Means clustering.  We compare Q implementations of these
benchmarks run by Parakeet to hand-written, tuned GPU and CPU implementations
from the Rodinia \cite{Che09} and Parsec \cite{Bien08} benchmark suites and the
NVIDIA CUDA SDK \cite{NvidSD}.  Despite the Q implementations requiring orders
of magnitude fewer lines of code, our system delivers competitive performance.

% Bullet-point, drive-home contributions
The main contributions of this paper are the following:

\begin{itemize}
\item A working system in which programmers can write complex, high level code
that is automatically parallelized into efficient GPU programs.
\item A series of high-level fusion optimization rules for higher-order data
parallel operators.
\item Evidence that dynamic compilation of GPU programs can significantly
improve their performance by taking advantage of runtime information to
tailor their code and execution to runtime information.
\end{itemize}


\section{Parakeet's Intermediate Language}
The portion of a user's program which Parakeet executes is represented internally as flat sequence of non-recursive function definitions. 
We take inspiration from \cite{Bol09} and allow functions to both accept and return multiple values. This feature both supports optimization (by simplifying the specification of arity-raising and fusion transformations) and naturally models the simultaneous creation of multiple values on the GPU. By convention we will write $\overline{\tau_m}$ to denote a sequence of $m$ types and $\overline{v_n}$ for a sequence  of $n$ values. Sequences of both types and values may be concatenated via juxtaposition, such that $\overline{\tau_m}, \overline{\tau_n} = \overline{\tau_{m+n}}$. 
\\[5pt]
\begin{tabular}{m{0.005cm}m{1.8cm}m{0.05cm}m{0.2cm}p{4.8cm}}
\multicolumn{2}{l}{\textbf{Type System}} & & &  \\
& data     & $\tau$    & $\bnfdef$ & $int \sep float \sep \mathbf{vec} \; \tau   $ \\[4pt]
& closures        & $\theta$  & $\bnfdef$ & $\overline{\tau_{c}} \Rightarrow \overline{\tau_m} \rightarrow \overline{\tau_n}$\\[2pt]
&                 &           & $\sep$    & $\tau$ \\[4pt]
& higher-order    & $\sigma$  & $\bnfdef$ & $\overline{\theta_m} \rightarrow \overline{\theta_n} $ \\[4pt]
\end{tabular}
\begin{itemize}
\item It is worth noting that the programmer is limited to defining values with first-order types (data or functions which neither accept nor return function values). It is, however, possible to make use of primitive second-order operators such as $\mathbf{map}$ and $\mathbf{reduce}$. 
\item The accepted typing of closures involves existential types [find citation for typed closure conversion?], which are commonly implemented via boxed . Since our desire is to compile down to a final program without boxed values we instead annotate closures with the types of their implicitly carried arguments. This information is later used to transform closure invocation into a direct function call. 
\end{itemize}
\begin{tabular}{m{0.1cm}m{1.2cm}m{0.1cm}m{0.2cm}p{4.7cm}}
 \multicolumn{2}{l}{\textbf{Language Syntax}} & & &  \\
& program & $p$ &  $\bnfdef$   &  $d_1 \cdots d_n $ \\[3.5pt]
& definition & $d$ & $\bnfdef$ & $f_i(\overline{x_m} : \overline{\tau_m}) \rightarrow (\overline{y_n} : \overline{\tau_n}) =s^{\small{+}}$ \\[3.5pt]
& statement  & $s$ & $\bnfdef$ & $\overline{x_m} : \overline{\tau_m} = e $\\[2pt]
&            &     & $\sep$    & $\IF \;v\; \THEN \;s^*\; \ELSE \;s^*$ \\[2pt]
&            &     & $\sep$    & $\WHILE \;e\; \DO \;s^+\;  $ \\[2pt]
& expression & $e$ & $\bnfdef$ & $ v_{f}(v_1, \ldots, v_m)$ \\[2pt]
&            &     & $\sep$    & $\textrm{values} (\overline{v_m})$ \\[2pt]
&            &     & $\sep$    & $\textrm{cast} \; (v, \tau)$ \\[2pt] 
&            &     & $\sep$    & $\MAP_m (v_{f},\overline{v_m})$ \\[2pt]
&            &     & $\sep$    & $\REDUCE_m (v_{f}, v_{init}, \overline{v_m})$ \\[2pt]
&            &     & $\sep$    & $\SCAN_m (v_{f}, v_{init}, \overline{v_m})$ \\[3.5pt]
& value      & $v$ & $\bnfdef$ & numeric constant\\[2pt]
&            &     & $\sep$    &  $x$  \quad \small{(data variable)} \\[2pt]
&            &     & $\sep$    &  $f$  \quad \small{(function label)} \\[2pt]
&            &     & $\sep$    &  $p$ \quad \small{(first order operator)} \\[2pt]
\end{tabular}
First-order primitives include the usual boolean operations, scalar and floating point arithmetic, as well as simple array operators such as indexing, replication, and inspecting an array's shape. 
\\TODO: 
\begin{itemize}
\item Gated SSA (reference to Program Dependence Web?), explain gates in one sentence. 
\item Mention that $\MAP$ and friends are actually a family of operators parameterized by arity. 
\item What to do with reduce? The stream idea is wonky and not included in the type system. 
\end{itemize} 

\section{Q: A High-Level Array Language}
\label{Q}

While we emphasize that the Parakeet framework is largely source language
agnostic, we chose to implement its first front end for Q, a high-level,
sequential array programming language from the APL family \cite{Borr08}.
Q is dynamically typed, and the standard proprietary implementation is
interpreted. Q is a natural choice for Parakeet, since idiomatic Q code uses
native array types and a rich set of array operators and higher-order
data parallel function modifiers that map well onto the Parakeet array
operators. Q is also a fully-featured language, with a large library of built-in
functions, and has a large user base in financial computing. Since our
focus is the Parakeet runtime, we omit many details of the Q language and only
present the salient features so as to illustrate the level of programming
and Q's support for array operators, as well as to make our later code examples
clear.


\subsection{Q Examples}

\begin{lstlisting}[caption=Q Map Example]
f: { [x] 5 + 3 * x}
/*/ Create input array*/
a: til 10000
/*/ Call f on different inputs*/
b: f[6]
c: f[a]
\end{lstlisting}
\label{QMap}

Figure \ref{QMap} gives an example of a simple Q program.  In line 1, we define
a function \texttt{f} which takes a single argument \texttt{x} and computes a
simple linear function of it.  In Q syntax, the colon operator is used for
assignment, and curly braces surround function definitions.  Brackets are used
to declare function arguments, with semicolons serving as list delimiters
between the various arguments.  Q functions return the result of their final
statement.  On line 3 we see a comment, denoted by a single \texttt{/}, and on
line 4 we use the built-in function \texttt{til} to generate a
100000-element array of integers ranging from \texttt{0} to \texttt{999999}.

In line 7, we finally perform our first computation, evaluating \texttt{f} on
the input \texttt{6} with the bracket function call syntax, storing the
value \texttt{23} in variable \texttt{b}.  Line 8 evaluates \texttt{f}
element-wise on the entire array \texttt{a}, storing the result in \texttt{c}.

This code snippet illustrates a key feature of Q for our purposes.  Functions
are agnostic to the shape of their input arguments--as we saw, \texttt{f} was
legally applied both to a scalar input as well as to an array.  In effect, the
function \texttt{f} was {\it mapped} element-wise over the array \texttt{a}.
Thus we see that Q programs can express data parallel computations in a compact,
declarative, high level fashion.  No explicit looping over the elements of
\texttt{a} is required.

\begin{lstlisting}[caption=Q Reductions Example]
a: til 10000
b: 0 +/ a
c: 0 +\ a
\end{lstlisting}
\label{QReduce}


In Figure \ref{QReduce}, we introduce the concept of an \emph{adverb}.  Q has a
series of adverbs, which are higher-order function modifiers.  In
line 3, we see an example of the reduction adverb, denoted with the syntax
`\texttt{/}', which modifies a binary function. When the modified function is
then applied to an array input, the result is as though the function was
inserted infix between all elements of the array.  Thus this adverb corresponds
to the \texttt{Reduce} array operator of the Parakeet language.  The reduce
adverb also takes an initial value argument, which is treated as though it were
prepended to the input of the reduce.  Thus, after executing line 3 \texttt{b}
will contain the sum of the array \texttt{a}.

In line 4, we see the \emph{scan} adverb, with syntax `\texttt{\char`\\}'.
Scan computes the running value of the function it modifies when applied
element-per-element to an input array. Scan also takes an initial value
argument.  In Parakeet, this corresponds to the \texttt{Scan} array operator,
which as emphasized in \cite{Blel90}, is very useful as a building block in many
parallel algorithms.

\begin{lstlisting}[caption=Q All-Pairs Distance Example]
dist: {[x;y] sqrt sum (x-y) * (x-y)}

/*/ Generate two 100 by 100 matrices*/
a: (100;100) # til 10000
b: (100;100) # til 10000
all_dists: dist/:\:[a;b]
\end{lstlisting}
\label{QAllPairsDist}

In Figure \ref{QAllPairsDist}, we see a slightly more complicated example.
First we define a pair of functions--one that computes the square of an input
variable, and another that computes the Euclidean distance of two input
vectors.  The \texttt{sum} function used in the \texttt{dist} function body is a
built-in reduction function, which computes the sum of all elements of its input
argument.  The \texttt{dist} function also illustrates that Q code is evaluated
\emph{left of right}--the input vectors \texttt{x} and \texttt{y} are
subtracted, then their square is computed, this is summed, and finally the
square root of this sum is returned.

In lines 5 and 6, we generate two 100x100 matrices.  The \texttt{\#} operator is
used to modify the shape of a variable: its left argument is a list of lengths
for the dimensions, and the right argument is a variable whose shape is to be
modified.  Lists in Q are enclosed in parentheses.

In line 8, we evaluate the \texttt{dist} function on the matrices \texttt{a} and
\texttt{b} using the {\it all pairs} adverb (corresponding to the
\texttt{AllPairs} Parakeet array operator).  This adverb--which is technically
a combination of the adverbs \emph{left-each} (\texttt{/:}) and
\emph{right-each} (\texttt{\char`\\:})--applies its function argument to all
pairs of each of its inputs--in this case, all pairs of rows from \texttt{a} and
rows from \texttt{b}.  The resulting \texttt{all\_dists} variable will contain a
matrix of all the Euclidean distances between rows of \texttt{a} and \texttt{b}.

We now turn to how Parakeet executes Q programs, including synthesizing GPU
kernels to accelerate them.

\section{Compilation Pipeline}
\label{Compilation}

Do All-Pairs distances here.

Here's some placeholder text for the compilation pipeline section.  This
section should easily take up the most of any section, around 2.5-3 pages I
would imagine.
Here's some placeholder text for the compilation pipeline section.  This
section should easily take up the most of any section, around 2.5-3 pages I
would imagine.Here's some placeholder text for the compilation pipeline section.
 This
section should easily take up the most of any section, around 2.5-3 pages I
would imagine.

Here's some placeholder text for the compilation pipeline section.  This
section should easily take up the most of any section, around 2.5-3 pages I
would imagine.
Here's some placeholder text for the compilation pipeline section.  This
section should easily take up the most of any section, around 2.5-3 pages I
would imagine.Here's some placeholder text for the compilation pipeline section.
 This
section should easily take up the most of any section, around 2.5-3 pages I
would imagine.
Here's some placeholder text for the compilation pipeline section.  This
section should easily take up the most of any section, around 2.5-3 pages I
would imagine.
Here's some placeholder text for the compilation pipeline section.  This
section should easily take up the most of any section, around 2.5-3 pages I
would imagine.Here's some placeholder text for the compilation pipeline section.
 This
section should easily take up the most of any section, around 2.5-3 pages I
would imagine.

Here's some placeholder text for the compilation pipeline section.  This
section should easily take up the most of any section, around 2.5-3 pages I
would imagine.
Here's some placeholder text for the compilation pipeline section.  This
section should easily take up the most of any section, around 2.5-3 pages I
would imagine.Here's some placeholder text for the compilation pipeline section.
 This
section should easily take up the most of any section, around 2.5-3 pages I
would imagine.
Here's some placeholder text for the compilation pipeline section.  This
section should easily take up the most of any section, around 2.5-3 pages I
would imagine.
Here's some placeholder text for the compilation pipeline section.  This
section should easily take up the most of any section, around 2.5-3 pages I
would imagine.Here's some placeholder text for the compilation pipeline section.
 This
section should easily take up the most of any section, around 2.5-3 pages I
would imagine.

Here's some placeholder text for the compilation pipeline section.  This
section should easily take up the most of any section, around 2.5-3 pages I
would imagine.
Here's some placeholder text for the compilation pipeline section.  This
section should easily take up the most of any section, around 2.5-3 pages I
would imagine.Here's some placeholder text for the compilation pipeline section.
 This
section should easily take up the most of any section, around 2.5-3 pages I
would imagine.
Here's some placeholder text for the compilation pipeline section.  This
section should easily take up the most of any section, around 2.5-3 pages I
would imagine.
Here's some placeholder text for the compilation pipeline section.  This
section should easily take up the most of any section, around 2.5-3 pages I
would imagine.Here's some placeholder text for the compilation pipeline section.
 This
section should easily take up the most of any section, around 2.5-3 pages I
would imagine.
Here's some placeholder text for the compilation pipeline section.  This
section should easily take up the most of any section, around 2.5-3 pages I
would imagine.
Here's some placeholder text for the compilation pipeline section.  This
section should easily take up the most of any section, around 2.5-3 pages I
would imagine.Here's some placeholder text for the compilation pipeline section.
 This
section should easily take up the most of any section, around 2.5-3 pages I
would imagine.
Here's some placeholder text for the compilation pipeline section.  This
section should easily take up the most of any section, around 2.5-3 pages I
would imagine.
Here's some placeholder text for the compilation pipeline section.  This
section should easily take up the most of any section, around 2.5-3 pages I
would imagine.Here's some placeholder text for the compilation pipeline section.
 This
section should easily take up the most of any section, around 2.5-3 pages I
would imagine.

Here's some placeholder text for the compilation pipeline section.  This
section should easily take up the most of any section, around 2.5-3 pages I
would imagine.
Here's some placeholder text for the compilation pipeline section.  This
section should easily take up the most of any section, around 2.5-3 pages I
would imagine.Here's some placeholder text for the compilation pipeline section.
 This
section should easily take up the most of any section, around 2.5-3 pages I
would imagine.
Here's some placeholder text for the compilation pipeline section.  This
section should easily take up the most of any section, around 2.5-3 pages I
would imagine.
Here's some placeholder text for the compilation pipeline section.  This
section should easily take up the most of any section, around 2.5-3 pages I
would imagine.Here's some placeholder text for the compilation pipeline section.
 This
section should easily take up the most of any section, around 2.5-3 pages I
would imagine.

Here's some placeholder text for the compilation pipeline section.  This
section should easily take up the most of any section, around 2.5-3 pages I
would imagine.
Here's some placeholder text for the compilation pipeline section.  This
section should easily take up the most of any section, around 2.5-3 pages I
would imagine.Here's some placeholder text for the compilation pipeline section.
 This
section should easily take up the most of any section, around 2.5-3 pages I
would imagine.
Here's some placeholder text for the compilation pipeline section.  This
section should easily take up the most of any section, around 2.5-3 pages I
would imagine.
Here's some placeholder text for the compilation pipeline section.  This
section should easily take up the most of any section, around 2.5-3 pages I
would imagine.Here's some placeholder text for the compilation pipeline section.
 This
section should easily take up the most of any section, around 2.5-3 pages I
would imagine.

Here's some placeholder text for the compilation pipeline section.  This
section should easily take up the most of any section, around 2.5-3 pages I
would imagine.
Here's some placeholder text for the compilation pipeline section.  This
section should easily take up the most of any section, around 2.5-3 pages I
would imagine.Here's some placeholder text for the compilation pipeline section.
 This
section should easily take up the most of any section, around 2.5-3 pages I
would imagineHere's some placeholder text for the compilation pipeline section.
This
section should easily take up the most of any section, around 2.5-3 pages I
would imagine.
Here's some placeholder text for the compilation pipeline section.  This
section should easily take up the most of any section, around 2.5-3 pages I
would imagine.Here's some placeholder text for the compilation pipeline section.
 This
section should easily take up the most of any section, around 2.5-3 pages I
would imagine.
Here's some placeholder text for the compilation pipeline section.  This
section should easily take up the most of any section, around 2.5-3 pages I
would imagine.
Here's some placeholder text for the compilation pipeline section.  This
section should easily take up the most of any section, around 2.5-3 pages I
would imagine.Here's some placeholder text for the compilation pipeline section.
 This
section should easily take up the most of any section, around 2.5-3 pages I
would imagine.

Here's some placeholder text for the compilation pipeline section.  This
section should easily take up the most of any section, around 2.5-3 pages I
would imagine.
Here's some placeholder text for the compilation pipeline section.  This
section should easily take up the most of any section, around 2.5-3 pages I
would imagine.Here's some placeholder text for the compilation pipeline section.
 This
section should easily take up the most of any section, around 2.5-3 pages I
would imagine.
Here's some placeholder text for the compilation pipeline section.  This
section should easily take up the most of any section, around 2.5-3 pages I
would imagine.
Here's some placeholder text for the compilation pipeline section.  This
section should easily take up the most of any section, around 2.5-3 pages I
would imagine.Here's some placeholder text for the compilation pipeline section.
 This
section should easily take up the most of any section, around 2.5-3 pages I
would imagine.
Here's some placeholder text for the compilation pipeline section.  This
section should easily take up the most of any section, around 2.5-3 pages I
would imagine.
Here's some placeholder text for the compilation pipeline section.  This
section should easily take up the most of any section, around 2.5-3 pages I
would imagine.Here's some placeholder text for the compilation pipeline section.
 This
section should easily take up the most of any section, around 2.5-3 pages I
would imagine.
Here's some placeholder text for the compilation pipeline section.  This
section should easily take up the most of any section, around 2.5-3 pages I
would imagine.
Here's some placeholder text for the compilation pipeline section.  This
section should easily take up the most of any section, around 2.5-3 pages I
would imagine.Here's some placeholder text for the compilation pipeline section.
 This
section should easily take up the most of any section, around 2.5-3 pages I
would imagine.

Here's some placeholder text for the compilation pipeline section.  This
section should easily take up the most of any section, around 2.5-3 pages I
would imagine.
Here's some placeholder text for the compilation pipeline section.  This
section should easily take up the most of any section, around 2.5-3 pages I
would imagine.Here's some placeholder text for the compilation pipeline section.
 This
section should easily take up the most of any section, around 2.5-3 pages I
would imagine
Here's some placeholder text for the compilation pipeline section.  This
section should easily take up the most of any section, around 2.5-3 pages I
would imagine.
Here's some placeholder text for the compilation pipeline section.  This
section should easily take up the most of any section, around 2.5-3 pages I
would imagine.Here's some placeholder text for the compilation pipeline section.
 This
section should easily take up the most of any section, around 2.5-3 pages I
would imagine.
Here's some placeholder text for the compilation pipeline section.  This
section should easily take up the most of any section, around 2.5-3 pages I
would imagine.
Here's some placeholder text for the compilation pipeline section.  This
section should easily take up the most of any section, around 2.5-3 pages I
would imagine.Here's some placeholder text for the compilation pipeline section.
 This
section should easily take up the most of any section, around 2.5-3 pages I
would imagine.

Here's some placeholder text for the compilation pipeline section.  This
section should easily take up the most of any section, around 2.5-3 pages I
would imagine.
Here's some placeholder text for the compilation pipeline section.  This
section should easily take up the most of any section, around 2.5-3 pages I
would imagine.Here's some placeholder text for the compilation pipeline section.
 This
section should easily take up the most of any section, around 2.5-3 pages I
would imagine.
Here's some placeholder text for the compilation pipeline section.  This
section should easily take up the most of any section, around 2.5-3 pages I
would imagine.
Here's some placeholder text for the compilation pipeline section.  This
section should easily take up the most of any section, around 2.5-3 pages I
would imagine.Here's some placeholder text for the compilation pipeline section.
 This
section should easily take up the most of any section, around 2.5-3 pages I
would imagine.Here's some placeholder text for the compilation pipeline section.
 This
section should easily take up the most of any section, around 2.5-3 pages I
would imagine.
Here's some placeholder text for the compilation pipeline section.  This
section should easily take up the most of any section, around 2.5-3 pages I
would imagine.Here's some placeholder text for the compilation pipeline section.
 This
section should easily take up the most of any section, around 2.5-3 pages I
would imagine.
Here's some placeholder text for the compilation pipeline section.  This
section should easily take up the most of any section, around 2.5-3 pages I
would imagine.
Here's some placeholder text for the compilation pipeline section.  This
section should easily take up the most of any section, around 2.5-3 pages I
would imagine.Here's some placeholder text for the compilation pipeline section.
 This
section should easily take up the most of any section, around 2.5-3 pages I
would imagine.

Here's some placeholder text for the compilation pipeline section.  This
section should easily take up the most of any section, around 2.5-3 pages I
would imagine.
Here's some placeholder text for the compilation pipeline section.  This
section should easily take up the most of any section, around 2.5-3 pages I
would imagine.Here's some placeholder text for the compilation pipeline section.
 This
section should easily take up the most of any section, around 2.5-3 pages I
would imagine
Here's some placeholder text for the compilation pipeline section.  This
section should easily take up the most of any section, around 2.5-3 pages I
would imagine.
Here's some placeholder text for the compilation pipeline section.  This
section should easily take up the most of any section, around 2.5-3 pages I
would imagine.Here's some placeholder text for the compilation pipeline section.
 This
section should easily take up the most of any section, around 2.5-3 pages I
would imagine.
Here's some placeholder text for the compilation pipeline section.  This
section should easily take up the most of any section, around 2.5-3 pages I
would imagine.
Here's some placeholder text for the compilation pipeline section.  This
section should easily take up the most of any section, around 2.5-3 pages I
would imagine.Here's some placeholder text for the compilation pipeline section.
 This
section should easily take up the most of any section, around 2.5-3 pages I
would imagine.

Here's some placeholder text for the compilation pipeline section.  This
section should easily take up the most of any section, around 2.5-3 pages I
would imagine.
Here's some placeholder text for the compilation pipeline section.  This
section should easily take up the most of any section, around 2.5-3 pages I
would imagine.Here's some placeholder text for the compilation pipeline section.
 This
section should easily take up the most of any section, around 2.5-3 pages I
would imagine.
Here's some placeholder text for the compilation pipeline section.  This
section should easily take up the most of any section, around 2.5-3 pages I
would imagine.
Here's some placeholder text for the compilation pipeline section.  This
section should easily take up the most of any section, around 2.5-3 pages I
would imagine.Here's some placeholder text for the compilation pipeline section.
 This
section should easily take up the most of any section, around 2.5-3 pages I
would imagine.
Here's some placeholder text for the compilation pipeline section.  This
section should easily take up the most of any section, around 2.5-3 pages I
would imagine.
Here's some placeholder text for the compilation pipeline section.  This
section should easily take up the most of any section, around 2.5-3 pages I
would imagine.Here's some placeholder text for the compilation pipeline section.
 This
section should easily take up the most of any section, around 2.5-3 pages I
would imagine.
Here's some placeholder text for the compilation pipeline section.  This
section should easily take up the most of any section, around 2.5-3 pages I
would imagine.
Here's some placeholder text for the compilation pipeline section.  This
section should easily take up the most of any section, around 2.5-3 pages I
would imagine.Here's some placeholder text for the compilation pipeline section.
 This
section should easily take up the most of any section, around 2.5-3 pages I
would imagine.

Here's some placeholder text for the compilation pipeline section.  This
section should easily take up the most of any section, around 2.5-3 pages I
would imagine.
Here's some placeholder text for the compilation pipeline section.  This
section should easily take up the most of any section, around 2.5-3 pages I
would imagine.Here's some placeholder text for the compilation pipeline section.
 This
section should easily take up the most of any section, around 2.5-3 pages I
would imagine
Here's some placeholder text for the compilation pipeline section.  This
section should easily take up the most of any section, around 2.5-3 pages I
would imagine.
Here's some placeholder text for the compilation pipeline section.  This
section should easily take up the most of any section, around 2.5-3 pages I
would imagine.Here's some placeholder text for the compilation pipeline section.
 This
section should easily take up the most of any section, around 2.5-3 pages I
would imagine.
Here's some placeholder text for the compilation pipeline section.  This
section should easily take up the most of any section, around 2.5-3 pages I
would imagine.
Here's some placeholder text for the compilation pipeline section.  This
section should easily take up the most of any section, around 2.5-3 pages I
would imagine.Here's some placeholder text for the compilation pipeline section.
 This
section should easily take up the most of any section, around 2.5-3 pages I
would imagine.

Here's some placeholder text for the compilation pipeline section.  This
section should easily take up the most of any section, around 2.5-3 pages I
would imagine.
Here's some placeholder text for the compilation pipeline section.  This
section should easily take up the most of any section, around 2.5-3 pages I
would imagine.Here's some placeholder text for the compilation pipeline section.
 This
section should easily take up the most of any section, around 2.5-3 pages I
would imagine.
Here's some placeholder text for the compilation pipeline section.  This
section should easily take up the most of any section, around 2.5-3 pages I
would imagine.
Here's some placeholder text for the compilation pipeline section.  This
section should easily take up the most of any section, around 2.5-3 pages I
would imagine.Here's some placeholder text for the compilation pipeline section.
 This
section should easily take up the most of any section, around 2.5-3 pages I
would imagine.

Here's some placeholder text for the compilation pipeline section.  This
section should easily take up the most of any section, around 2.5-3 pages I
would imagine.
Here's some placeholder text for the compilation pipeline section.  This
section should easily take up the most of any section, around 2.5-3 pages I
would imagine.Here's some placeholder text for the compilation pipeline section.
 This
section should easily take up the most of any section, around 2.5-3 pages I
would imagine.
Here's some placeholder text for the compilation pipeline section.  This
section should easily take up the most of any section, around 2.5-3 pages I
would imagine.
Here's some placeholder text for the compilation pipeline section.  This
section should easily take up the most of any section, around 2.5-3 pages I
would imagine.Here's some placeholder text for the compilation pipeline section.
 This
section should easily take up the most of any section, around 2.5-3 pages I
would imagine.

Here's some placeholder text for the compilation pipeline section.  This
section should easily take up the most of any section, around 2.5-3 pages I
would imagine.
Here's some placeholder text for the compilation pipeline section.  This
section should easily take up the most of any section, around 2.5-3 pages I
would imagine.Here's some placeholder text for the compilation pipeline section.
 This
section should easily take up the most of any section, around 2.5-3 pages I
would imagine.
Here's some placeholder text for the compilation pipeline section.  This
section should easily take up the most of any section, around 2.5-3 pages I
would imagine.
Here's some placeholder text for the compilation pipeline section.  This
section should easily take up the most of any section, around 2.5-3 pages I
would imagine.Here's some placeholder text for the compilation pipeline section.
 This
section should easily take up the most of any section, around 2.5-3 pages I
would imagine.

Here's some placeholder text for the compilation pipeline section.  This
section should easily take up the most of any section, around 2.5-3 pages I
would imagine.
Here's some placeholder text for the compilation pipeline section.  This
section should easily take up the most of any section, around 2.5-3 pages I
would imagine.Here's some placeholder text for the compilation pipeline section.
 This
section should easily take up the most of any section, around 2.5-3 pages I
would imagine.

\subsection{CUDA Back End}

In this section we give an overview of modern GPU architecture and the CUDA
programming model.  Modern graphics processors are made up of hierarchical
arrays of simple processors, with each level of the hierarchy sharing some
commmon hardware resources.  When performing graphics computations these
processors execute shader programs for performing rendering tasks.  With the
advent of GPGPU programming environments such as NVIDIA's CUDA \cite{NvidCU} and
OpenCL \cite{Muns10}, programmers can execute general-purpose programs directly
on these processors as well.

The current back end for Parakeet targets the CUDA platform by emitting PTX, the
pseudoassembly language in which CUDA programs are ultimately implemented.
In CUDA, a program is organized into {\it kernels} and {\it host} code.
Kernels are sequential code that is run by typically many thousands of ultra
lightweight threads on the GPU's, or {\it device}'s, processors.  Host code is
the code run on the CPU. Typically, a host thread launches a kernel onto the
GPU, waits for results, and then potentially launches further kernels.

A CUDA kernel's threads are organized into {\it thread blocks}, which are groups
of at most a few hundred threads that can communicate and share local resources.
Threads within a block can perform barrier synchronizations with each other via
a CUDA intrinsic, but threads from different blocks have no way to perform
synchronization other than for the kernel to terminate.  Thus algorithms that
require global synchronization must be broken up into multiple kernel launches,
each of which incurs a small fixed performance overhead.  The body of a
kernel can be written so as to be agnostic to the number of threads and
blocks with which it is launched, with the programmer specifying at runtime how
many thread blocks are required and how many threads to launch per block. This
allows the kernel programs to be scalable to different datasizes without
requiring them to be rewritten. The standard idiom is for each thread to be
assigned a small computation to be performed on a small subset of some large
input data set.  The threads are each assigned a unique index in the kernel's
{\it grid} of threads, which is used to determine the data for which they are
responsible.

The GPU consists of a series--typically a few hundred--simple processors that
are organized into small groups (on current chips between 8 and 48) of
processors called {\it symmetric multiprocessors} or SMs.  SMs form the basic
hardware scheduling unit, with processors within an SM sharing instruction
fetch and decode units and thread blocks executing in their entirety on a
single SM.

At the lowest level of the processor hierarchy are symmetric
processors or SPs, which are very simple sequential processors.  At the
next level are symmetric multiprocessors or SMs, which are groups of 8 SPs that
share an instruction issue and decode unit.  This means that all SPs within an
SM execute their instructions in lockstep.  In the CUDA model, thread blocks are
scheduled to run in their entirety on a single SM.  The SPs are then grouped to
share texture processing units, and then these groups along with L2 caches make
up the GPU chip.  The graphics card bundles the GPU along with a large block of
off-chip DRAM, typically on the order of a few gigabytes in modern cards.

The CUDA model is a Single Instruction Multiple Thread or SIMT model, meaning
that multiple threads with a block each execute the same instruction at the same
time.  This differs from SIMD models in that branching instructions are allowed
which diverge with a thread block.  However, this results in a significant
performance degradation, as all threads in the block are then forced to execute
both paths of the branch, with logical noops being executed by the threads on
the path which didn't satisfy their conditional test.

In order to get maximum performance on a GPU, it is very important to manage
memory usage.  While the peak memory bandwidth on modern GPUs is up to 150GB/sec
or more--up to an order of magnitude higher than that of CPUs--this bandwidth
can only be reached with careful access patterns on the part of threads within a
block.  In addition, each SM has a 16KB scratchpad called {\it shared memory}.
The programmer must explicitly manage shared memory, and careful usage of it can
result in orders of magnitude performance improvements over naively-written CUDA
programs.  Further, data must be manually moved between system RAM and the
graphics card's DRAM.  Modern GPUs use PCI-Express, which has a maximum transfer
rate of around 4GB/sec, which can easily become the major bottleneck in CUDA
programs if memory transfers aren't managed carefully.  Finally, graphics cards
tend to have limited global memory (typically between 500MB and 2GB), and a
user's windowing system can consume much of this on its own (on our office
desktops, X Windows tends to consume around 600MB).  Thus GPU RAM is an
important scarce resource to be managed.

Various constraints limit how a programmer can structure his or her CUDA
programs.  Each SM is able to execute a maximum of 8 thread blocks and
a maximum of 768-1536 threads at a time.  In addition, a group of thread blocks
can be co-scheduled on an SM only if they consume at most 8K-32K registers, as
each SM shares a register pool.  Finally, no more than 16-48KB of shared
memory per SM can be used at a time, and so thread blocks which in aggregate
exceed this limit cannot be co-scheduled.

The complexity of programming in the CUDA platform should be evident.  Parakeet
aims to manage these low level details efficiently and transparently for the
programmer.

%\subsection{GPU Memory Management}
%
%When
%Parakeet synthesizes a GPU program, it first determines whether the input and
%output arguments for that program will fit in their entirety in the available
%GPU memory.  Typically, GPUs have around 500MB to 2GB of RAM, and a significant
%portion of this can be taken up by a user's windowing system.  (In our
%experiments, on a card with 896MB of memory, X Windows can consume well over
%600MB on its own during simple office usage.)  If the data does fit, Parakeet
%copies whatever inputs are not already on the GPU to it and allocates storage
%for the output.  If not, Parakeet chunks the data up into manageable portions,
%copying down one section at a time to the GPU, and then transfering the
%partial result back to the CPU to build the complete result.

\section{Evaluation}
\label{Evaluation}

We evaluated Parakeet on two standard benchmark programs: Black-Scholes option
pricing, and K-Means Clustering.  We compare Parakeet against both hand-tuned
CPU and GPU implementations.  For Black-Scholes, the CPU implementation is
taken from the PARSEC \cite{Bien08} benchmark suite--which we used as the basis
of our Q implementation--and the GPU implementation is taken from the CUDA SDK
\cite{NvidSD}.  For K-Means Clustering, we wrote our own Q version in 17 lines
of code.  Both the CPU and GPU benchmark version come from the Rodinia
benchmark suite \cite{Che09}.

Our experimental setup is as follows.  We ran the CPU benchmarks on a machine
with 3 Intel Nehalem 2.67GHz X5650 4-core CPUs (for a total of 12 cores) with
24GB of RAM.  The peak throughput of this machine is thus 32GLOP/s, and it has
a peak memory bandwidth of 32GB/s.

For all GPU benchmarks, we ran the programs on both an NVIDIA GTX2XX and an
NVIDIA GTX460.  The GTX2XX has 240 processor cores with clock speeds of
Y.YY GHz and YMB of memory, with peak execution throughput of Y.YY GFLOP/s and
peak memory bandwidth of Y.YY GB/s.  The GTX460 has 336 processor cores with
clock speeds of 1.35GHz and 1GB of memory, and has a peak execution throughput
of 907 GFLOP/s and a peak memory bandwidth of 115.2 GB/s.

For our Parakeet benchmarks, the desktop system we used had a BLAH processor
with BLAH specs.

\subsection{Black-Scholes}
Black-Scholes option pricing \cite{Blac73} is a standard benchmark for data
parallel workloads, since it is embarrassingly parallel--the calculation of the
price of a given option doesn't impact that of any other, and the benchmark
consists of simply running thousands of independent threads in parallel for
computing the prices of thousands of options.

We compare our system against the multithreaded OpenMP CPU implementation
from the PARSEC \cite{Bien08} benchmark suite for 1 to 12 threads and 

\subsection{K-Means Clustering}

\begin{lstlisting}[caption=K-Means Clustering implemented in Q]
calc_centroid:{[X;a;i] avg X[where a = i]}
calc_centroids:{[X;a;k] 
  calc_centroid[X;a] each til k}
  
dist:{[x;y] sqrt sum (x-y) * (x-y)}
minidx:{[x] x ? min x}

kmeans:{[X;k;a]
  C: calc_centroids[X;a;k];
  converged: 0b;
  while[not converged;
    lastAssignment: a;
    D: X dist/:\: C;
    a: minidx each D;
    C: calc_centroids[X;a;k];
    converged: all lastAssignment = a];
  C}
\end{lstlisting}
\label{QKMeans}

\subsection{Dynamic Tuning}
In order to test the effects of our dynamic tuning optimizations, 

\section{Related Work}
\label{RelatedWork}

There's Copperhead \cite{Cata10}.  They're almost as good as we are.

There has been much recent work dedicated to making parallel programming easier.
The various approaches to addressing this problem include libraries and APIs
which provide parallel functionality as well as new programming languages meant
for parallel programming.

Skeletons are cool \cite{Cole04}.

Recent work on parallel programming languages includes Fortress \cite{Alle08},
Unified Parallel C \cite{Char05}, and X10 \cite{Chen05}.  Our approach differs
from all of these in two important ways.  First, we are focusing on compiler
techniques for sequential array programming languages which do not require the
programmer to explicitly manage parallelism.  These techniques are meant to be
generally applicable to any suitable array language, not just Q.  Second, our
approach doesn't introduce a new programming language specifically for writing
GPGPU or parallel programs.  Q is already in wide use in the financial computing
community.

There has also been other work directed specifically at raising the level of
abstraction above that of CUDA and OpenCL for doing GPGPU programming.  Conal
Elliot's Vertigo \cite{Elli04} was a declarative parallel language embedded in
Haskell which was used for GPGPU programming.  It was designed for older GPUs
(pre-CUDA), and only parallelized map computations (not handling, for example,
parallel reductions or all-pairs type computations).  Microsoft's Accelerator
\cite{Tard06} is a declarative GPU language embedded in C\# which creates a
directed acyclic graph of LINQ operations--a collection of common data querying
operations such as filtering, grouping, and joining--and compiles them to
(pre-CUDA) shader programs.  Joel Svensson's Obsidian \cite{Sven08} is another
declarative language embedded in Haskell which compiles at runtime to the GPU.
The language semantics are inspired by hardware description languages and while
it is a higher level of abstraction than directly coding CUDA, it is still much
lower level than an array language such as Q.  Finally, Jacket by Accelereyes
\cite{AcceJa} provides a similar setup for Matlab as we do for Q.  However,
Jacket requires programmers to explicitly declare data and computation to reside
on the GPU, whereas our framework takes advantage of the GPU completely
transparently to the programmer.

Our approach is very similar in spirit to that of the skeletal programming
community \cite{Cole04}.  We both seek to use generic parallel programming
constructs which capture most of the common parallel patterns in a concise and
easy-to-use language.  In our specific case, these constructs are embodied by
Q's adverbs, which are second-order function modifiers.  We describe these
adverbs in more detail in the next section.

\section{Conclusion}
\label{Conclusion}

We have presented Parakeet, an intelligent runtime for executing high level
array-oriented programs on GPUs.  Parakeet alleviates programmers from having
to write code that explicitly manages low level architectural details, pushing
the level of abstraction for GPGPU programming up to the level of productivity
languages.  Parakeet automatically synthesizes GPU programs from data parallel
array operators, transparently running these programs and moving data back and
forth to the GPU's memory and performing GPU memory garbage collection.
Parakeet includes a series of optimizations to generate more efficient GPU
programs, including array operator fusion and the use of shared and texture
memory on the GPU.  Parakeet is a working system, in which complex programs can
be written and executed efficiently.  On two benchmark programs, Parakeet
delivers performance competitive with even hand-tuned CPU and GPU
implementations.

In future work, we hope to support more front ends and back ends.  At the
moment, we are building front ends for both Matlab and Python.  We envision
building a back end for multicore CPUs as well, likely targeting LLVM
\cite{Latt02}.

\bibliographystyle{acm}
\bibliography{../Parallelism}{}

\end{document}
